{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优缺点\n",
    "根据http://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "## Some advantages of decision trees are:\n",
    "Simple to understand and to interpret. Trees can be visualised.\n",
    "Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n",
    "Able to handle multi-output problems.\n",
    "Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "## The disadvantages of decision trees include:\n",
    "Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sci-kit learn\n",
    "## 代码实现\n",
    "\n",
    "从https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/tree.py\n",
    "可以看到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CRITERIA_CLF = {\"gini\": _criterion.Gini, \"entropy\": _criterion.Entropy}\n",
    "CRITERIA_REG = {\"mse\": _criterion.MSE, \"friedman_mse\": _criterion.FriedmanMSE}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两行对应的分别是分类树和回归树进行split时的criteria。所以sklearn中支持的分类的criteria是gini和entropy，支持的回归的criteria则是mse，当然还有一个friedman提出来的mse（有啥区别？待查）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类树的生成是比较简单的：对于当前的分裂节点，遍历所有的feature，看哪个feature分裂的结果能够得到最大的gini系数或者entropy。Gini和Entropy都是用来描述变量的不确定性，它们的值越大，则表明变量的不确定性越大，也就是说变量取得各种值的概率相近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归树的生成还是有那么一点小麻烦的，是采用的启发式的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在sklearn中的tree的split有两种，dense和sparse，这个指的是输入的矩阵是sparse的还是dense的，跟算法本身没有关系。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn中的ExtraTreeRegressor继承了DecisionTreeRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    class sklearn.tree.DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, presort=False)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class sklearn.tree.ExtraTreeRegressor(criterion='mse', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, max_leaf_nodes=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准的DecisionTreeRegressor的splitter是best策略，ExtraDecisionTree继承了标准的DecisionTreeRegressor，主要的改变就是参数传递是将splitter设置成了random。ExtraTreeRegressor中的Extra其实是extremely random的简称。什么意思呢？对于标准的regression tree，在每一个节点进行分裂的时候，与分类树类似，会遍历每一个feature，然后对于每一个feature，遍历其所取的所有的值，然后计算mse，取mse最小的作为最好的分割。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是ExtraTreeRegressor则不同，同样的，它会遍历所有的feature，但是对于每一个feature，并不会遍历所有的取值，而是随机选择一个值作为分割值。相关代码为（其中，rand_uniform就是从当前feature的取值范围随机选择一个值）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "                    current.threshold = rand_uniform(min_feature_value,\n",
    "                                                     max_feature_value,\n",
    "                                                     random_state)\n",
    "\n",
    "                    if current.threshold == max_feature_value:\n",
    "                        current.threshold = min_feature_value\n",
    "\n",
    "                    # Partition\n",
    "                    partition_end = end\n",
    "                    p = start\n",
    "                    while p < partition_end:\n",
    "                        current_feature_value = Xf[p]\n",
    "                        if current_feature_value <= current.threshold:\n",
    "                            p += 1\n",
    "                        else:\n",
    "                            partition_end -= 1\n",
    "\n",
    "                            Xf[p] = Xf[partition_end]\n",
    "                            Xf[partition_end] = current_feature_value\n",
    "\n",
    "                            tmp = samples[partition_end]\n",
    "                            samples[partition_end] = samples[p]\n",
    "                            samples[p] = tmp\n",
    "\n",
    "                    current.pos = partition_end\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn对ExtraTreeRegressor的解释如下：Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the max_features randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n",
    "\n",
    "其中，特别强调：\n",
    "\n",
    "Warning: Extra-trees should only be used within ensemble methods.\n",
    "\n",
    "这是因为，如果只有一棵树，采用这种随机的方法生成的回归树显然mse会是很大的，离最优的mse可能会有相当的距离。但是如果是ensemble一堆的树，则分割值本身的随机性会解决这个问题。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
