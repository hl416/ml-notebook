{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优缺点\n",
    "根据http://curtis.ml.cmu.edu/w/courses/index.php/Stochastic_Gradient_Descent\n",
    "\n",
    "## 优点\n",
    "When this method is used for very large data sets that has redundant information among examples, it is much faster than the plain gradient descent because it requires less computation each iteration. Also, it is known to be better with noisy data since it samples example to compute gradient.\n",
    "#### 数据规模大\n",
    "#### 性能要求高，例如流数据，或者online的计算，比如电商等\n",
    "#### 数据存在噪音，这样可以sampling。当然也许micro-batch更好\n",
    "\n",
    "## 缺点\n",
    "The convergence rate is slower than second-order gradient methods. However the speedup coming from computationally efficient iterations are usually greater and the method can converge faster if learning rate is adjusted as the procedure goes on. Also it tends to keep bouncing around the minimum unless the learning rate is reduced in the later iterations.\n",
    "#### 算法收敛快（只是说迭代步骤少，并不是说时间短）\n",
    "#### \n",
    "\n",
    "# 虽然说是随机的，但是按照某种规律效果会更好吗，比如说最开始选一个什么sample\n",
    "http://www.nowcoder.com/questionTerminal/8bf94533d85349859b0e11c9857fedd5?orderByHotValue=0&done=0&pos=28&mutiTagIds=631&onlyReference=false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
